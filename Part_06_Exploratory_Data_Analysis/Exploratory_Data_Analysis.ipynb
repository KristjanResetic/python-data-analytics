{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full chapter [here](./Chapter_Visualizing_Data_Insights.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is a critical initial step in data analysis where researchers investigate datasets to understand their structure, patterns, and potential issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "dataset_path_clean_laptops = Path.cwd().parent / \"data\" / \"OUTPUT_laptops.parquet\"\n",
    "laptops = pd.read_parquet(dataset_path_clean_laptops)\n",
    "laptops.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics and Data Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics provide the foundation for understanding and interpreting data by summarizing its main features. When faced with a large dataset, it is often impractical—or even impossible—to examine every individual data point. Descriptive statistics condense the data into a few key measures that capture its central tendency, variability, and overall distribution. This subchapter focuses on the theory behind these techniques and explains why they are critical for effective data analysis.\n",
    "\n",
    "Data summarization is essential for several reasons:\n",
    "- **Simplification**: Large datasets are distilled into concise, interpretable numbers. Instead of sifting through thousands of records, analysts can review summary metrics.\n",
    "- **Insight Generation**: By calculating central tendencies (mean, median, mode) and dispersion measures (range, variance, standard deviation), one can quickly gauge the typical value in a dataset and understand how spread out the values are.\n",
    "- **Comparison**: Summarized statistics facilitate comparisons between different groups or over time, supporting informed decision-making.\n",
    "- **Foundation for Further Analysis**: Descriptive insights serve as a preliminary step before more complex inferential statistics or predictive modeling, helping to verify assumptions and detect anomalies in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Just like with NumPy, we can use any of the standard <a target=\"_blank\" href=\"https://docs.python.org/3/library/stdtypes.html#numeric-types-int-float-complex\">Python numeric operators</a> with series, including:</p>\n",
    "<ul>\n",
    "<li><code>series_a + series_b</code> - Addition</li>\n",
    "<li><code>series_a - series_b</code> - Subtraction</li>\n",
    "<li><code>series_a * series_b</code> - Multiplication (this is unrelated to the multiplications used in linear algebra).</li>\n",
    "<li><code>series_a / series_b</code> - Division</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<p>Pandas supports many descriptive stats methods that can help us answer these questions. Here are a few of the most useful ones (with links to documentation):</p>\n",
    "<ul>\n",
    "<li><a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.max.html\"><code>Series.max()</code></a> and <a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.max.html\"><code>DataFrame.max()</code></a></li>\n",
    "<li><a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.min.html\"><code>Series.min()</code></a> and <a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.min.html\"><code>DataFrame.min()</code></a></li>\n",
    "<li><a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.mean.html\"><code>Series.mean()</code></a> and <a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mean.html\"><code>DataFrame.mean()</code></a></li>\n",
    "<li><a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.median.html\"><code>Series.median()</code></a> and <a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.median.html\"><code>DataFrame.median()</code></a></li>\n",
    "<li><a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.mode.html\"><code>Series.mode()</code></a> and <a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mode.html\"><code>DataFrame.mode()</code></a></li>\n",
    "<li><a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sum.html\"><code>Series.sum()</code></a> and <a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html\"><code>DataFrame.sum()</code></a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of total storage vs. weight\n",
    "total_storage = laptops[\"storage_ssd_gb\"] + laptops[\"storage_hdd_gb\"] + laptops[\"storage_flash_gb\"] + laptops[\"storage_hybrid_gb\"]\n",
    "weight = laptops[\"weight_kg\"]\n",
    "mean_storage = total_storage.mean()\n",
    "mean_weight = weight.mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(total_storage, weight, alpha=0.5)\n",
    "plt.title(\"Total Storage vs. Weight of Laptops\")\n",
    "plt.xlabel(\"Total Storage (GB)\")\n",
    "plt.ylabel(\"Weight (kg)\")\n",
    "plt.grid(True)\n",
    "plt.xlim(0, 3000)  # Set x-axis limit to 3000 GB\n",
    "plt.ylim(0, 5)  # Set y-axis limit to 5 kg\n",
    "plt.xticks(np.arange(0, 3200, 200))\n",
    "plt.yticks(np.arange(0, 6, 0.5))\n",
    "plt.axhline(y=mean_weight, color=\"r\", linestyle=\"--\", label=\"Mean weight\")\n",
    "plt.axvline(x=mean_storage, color=\"g\", linestyle=\"--\", label=\"Mean storage\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Find the laptop with the highest RAM vs price ratio\n",
    "# Calculate the RAM vs price ratio\n",
    "laptops[\"ram_price_ratio\"] = laptops[\"ram_gb\"] / laptops[\"price_euros\"]\n",
    "# Find the laptop with the highest RAM vs price ratio\n",
    "highest_ram_price_ratio = laptops.loc[laptops[\"ram_price_ratio\"].idxmax()]\n",
    "print(\"Laptop with the highest RAM vs price ratio:\")\n",
    "highest_ram_price_ratio[[\"manufacturer\", \"model_name\", \"ram_gb\", \"price_euros\", \"ram_price_ratio\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html\"><code>Series.value_counts()</code> method</a>. This method displays each unique non-null value in a column and their counts in order.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of laptops by manufacturer\n",
    "laptops[\"cpu_manufacturer\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <p><strong>Method chaining</strong> —&nbsp;a way to combine multiple methods together in a single line.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the price median for the intel and amd laptops\n",
    "intel_price_median = laptops[laptops[\"cpu_manufacturer\"] == \"intel\"][\"price_euros\"].median()\n",
    "amd_price_median = laptops[laptops[\"cpu_manufacturer\"] == \"amd\"][\"price_euros\"].median()\n",
    "\n",
    "print(f\"Intel median price: {intel_price_median}\")\n",
    "print(f\"AMD median price: {amd_price_median}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Boolean indexing is a powerful tool which allows us to select or exclude parts of our data based on their values. However, to answer more complex questions, we need to learn how to combine boolean arrays.</p>\n",
    "<p>To recap, boolean arrays are created using any of the Python standard <strong>comparison operators</strong>: <code>==</code> (equal), <code>&gt;</code> (greater than), <code>&lt;</code> (less than), <code>!=</code> (not equal).</p>\n",
    "<p>We combine boolean arrays using <strong>boolean operators</strong>. In Python, these boolean operators are <code>and</code>, <code>or</code>, and <code>not</code>. In pandas, the operators are slightly different:</p>\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>pandas</th>\n",
    "<th>Python equivalent</th>\n",
    "<th>Meaning</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><code>a &amp; b</code></td>\n",
    "<td><code>a and b</code></td>\n",
    "<td><code>True</code> if both <code>a</code> and <code>b</code> are <code>True</code>, else <code>False</code></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>a | b</code></td>\n",
    "<td><code>a or b</code></td>\n",
    "<td><code>True</code> if either <code>a</code> or <code>b</code> is <code>True</code></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>~a</code></td>\n",
    "<td><code>not a</code></td>\n",
    "<td><code>True</code> if <code>a</code> is <code>False</code>, else <code>False</code></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 3 manufacturers with the most laptops with price > 1000 and nvidia gpus\n",
    "laptops_filtered = laptops[(laptops[\"price_euros\"] > 1000) & (laptops[\"gpu_manufacturer\"] == \"nvidia\")]\n",
    "top_3_manufacturers = laptops_filtered[\"manufacturer\"].value_counts().head(3)\n",
    "top_3_manufacturers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cheapest laptop with ssd or flash storage and at least 16GB of RAM\n",
    "laptops_filtered = laptops[(laptops[\"storage_ssd_gb\"] > 0) | (laptops[\"storage_flash_gb\"] > 0)]\n",
    "laptops_filtered = laptops_filtered[laptops_filtered[\"ram_gb\"] >= 16]\n",
    "cheapest_laptop = laptops_filtered.loc[laptops_filtered[\"price_euros\"].idxmin()]\n",
    "print(\"Cheapest laptop with SSD or flash storage and at least 16GB of RAM:\")\n",
    "cheapest_laptop[[\"manufacturer\", \"model_name\", \"ram_gb\", \"storage_ssd_gb\", \"storage_flash_gb\", \"price_euros\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all laptops from Asus that do not have a dedicated Nvidia GPU and have at least 16GB of RAM\n",
    "laptops_filtered = laptops[(laptops[\"manufacturer\"] == \"Asus\") & ~(laptops[\"gpu_manufacturer\"] == \"nvidia\") & (laptops[\"ram_gb\"] >= 16)]\n",
    "laptops_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Apple laptop with the highest price\n",
    "laptops[laptops[\"manufacturer\"] == \"Apple\"].sort_values(by=\"price_euros\", ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts laptops by manufacturer  only for laptops without HHD storage\n",
    "laptops[laptops[\"storage_hdd_gb\"] == 0][\"manufacturer\"].value_counts().head(10).plot(kind=\"barh\", figsize=(10, 6), color=\"skyblue\")\n",
    "plt.title(\"Top 10 Manufacturers without HDD Storage\")\n",
    "plt.xlabel(\"Number of Laptops\")\n",
    "plt.ylabel(\"Manufacturer\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy-on-Write (CoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Copy-on-Write (CoW)](https://pandas.pydata.org/docs/user_guide/copy_on_write.html#copy-on-write)\n",
    "- [Returning a view versus a copy](https://pandas.pydata.org/docs/user_guide/indexing.html#returning-a-view-versus-a-copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copy-on-Write will become the default in pandas 3.0. We recommend turning it on now to benefit from all improvements. Copy-on-Write was first introduced in version 1.5.0. Starting from version 2.0 most of the optimizations that become possible through CoW are implemented and supported. All possible optimizations are supported starting from pandas 2.1. **CoW will be enabled by default in version 3.0.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"full-width\" src=\"https://www.dataquest.io/wp-content/uploads/2019/01/view-vs-copy.png\" alt=\"view-vs-copy\">\n",
    "\n",
    "\n",
    "<img class=\"full-width\" src=\"https://www.dataquest.io/wp-content/uploads/2019/01/modifying.png\" alt=\"modifying\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoW will lead to more predictable behavior since it is not possible to update more than one object with one statement, e.g. indexing operations or methods won’t have side-effects. Additionally, through delaying copies as long as possible, the average performance and memory usage will improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previous behavior**\n",
    "\n",
    "pandas indexing behavior is tricky to understand. Some operations return views while other return copies. Depending on the result of the operation, mutating one object might accidentally mutate another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[\"foo\"]\n",
    "subset.iloc[0] = 100\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "Mutating subset, e.g. updating its values, also updates df. The exact behavior is hard to predict. Copy-on-Write solves accidentally modifying more than one object, it explicitly disallows this. With CoW enabled, df is unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]})\n",
    "subset = df[\"foo\"]\n",
    "subset.iloc[0] = 10\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy-on-Write will be the default and only mode in pandas 3.0. This means that users need to migrate their code to be compliant with CoW rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting values in a pandas object, care must be taken to avoid what is called chained indexing. Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6], \"zip\": [7, 8, 9]}, index=[\"a\", \"b\", \"c\"])\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access method 1:\n",
    "test_data[\"bar\"][\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access method 2:\n",
    "test_data.loc[\"b\", \"bar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These both yield the same results, so which should you use? It is instructive to understand the order of operations on these and why method 2 (.loc) is much preferred over method 1 (chained []).\n",
    "\n",
    "`test_data[\"bar\"]` selects the column and returns a Series. Then another Python operation `test_data_with_bar[\"b\"]` selects the element indexed by 'b'. This is indicated by the variable `test_data_with_bar` because pandas sees these operations as separate events. e.g. separate calls to `__getitem__`, so it has to treat them as linear operations, they happen one after another.\n",
    "\n",
    "Contrast this to `test_data.loc[\"b\", \"bar\"]` which make a single call to `__getitem__`. This allows pandas to deal with this as a single entity. Furthermore this order of operations can be significantly faster, and allows one to index both axes if so desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does assignment fail when using chained indexing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem in the previous section is just a performance issue. What’s up with the SettingWithCopy warning? We don’t usually throw warnings around when you do something that might cost a few extra milliseconds!\n",
    "\n",
    "But it turns out that assigning to the product of chained indexing has inherently unpredictable results. To see this, think about how the Python interpreter executes this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[\"b\", \"bar\"] = 100\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"bar\"][\"b\"] = 50\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside of simple cases, it’s very hard to predict whether it will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"bar\"][\"b\"] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the world of data analysis, you often work with large datasets containing many rows of detailed information. However, for decision-making or further analysis, you usually don’t need to inspect every individual record. Instead, you need to summarize or aggregate the data to:\n",
    "- Reveal Trends: Understand the overall performance (e.g., total sales, average ratings) rather than just individual data points.\n",
    "- Compare Groups: Compare different categories, such as sales by region or performance by department.\n",
    "- Simplify Analysis: Reduce the data to a manageable size while preserving essential patterns.\n",
    "- Enhance Reporting: Create meaningful summaries that are easy to visualize and interpret.\n",
    "\n",
    "Data aggregation helps to condense your dataset, making it easier to draw insights and take informed actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the groupby method, it’s important to understand the **split-apply-combine** strategy—a common paradigm in data analysis that underlies many aggregation techniques.\n",
    "- **Split**: Divide the dataset into groups based on one or more key variables (for example, grouping sales records by store).\n",
    "- **Apply**: Perform an operation on each group independently. This could be a statistical function like sum, mean, or even a custom transformation.\n",
    "- **Combine**: Merge the individual results from each group back into a single data structure.\n",
    "\n",
    "This strategy allows analysts to work on each group separately and then bring the results together in a concise summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To better understand what split-apply-combine is doing, here's a manual\n",
    "# implementation using a for loop.\n",
    "# Don't use this in practice, it's just for illustration.\n",
    "\n",
    "mean_prices = {}\n",
    "\n",
    "for laptop_category in laptops[\"category\"].unique():\n",
    "    category_group = laptops[laptops[\"category\"] == laptop_category]\n",
    "    mean_price = category_group[\"price_euros\"].mean()\n",
    "    mean_prices[laptop_category] = mean_price\n",
    "\n",
    "print(mean_prices)\n",
    "\n",
    "# Plot the mean prices for each category\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(mean_prices.keys(), mean_prices.values())\n",
    "plt.xlabel(\"Laptop Category\")\n",
    "plt.ylabel(\"Mean Price (Euros)\")\n",
    "plt.title(\"Mean Laptop Prices by Category\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas implements the split-apply-combine strategy using its powerful `groupby` method. Here’s what happens under the hood:\n",
    "- **Splitting the Data**: When you call `df.groupby('Column')`, pandas scans the specified column and divides the DataFrame into subsets, where each subset contains rows sharing the same value in that column.\n",
    "- **Applying a Function**: Once the data is split, you can apply a function to each group. This function could be an aggregation (like sum, mean, min, or max), a transformation (like scaling or normalizing), or even a filtering function. Pandas efficiently applies these operations to each subset.\n",
    "- **Combining the Results**: After the function is applied, pandas combines the output into a new DataFrame or Series. This new structure presents the aggregated results in a clear, tabular format.\n",
    "\n",
    "The elegance of groupby is that it abstracts away the need for explicit loops, providing a more efficient and readable approach to data aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have a dataset of sales records for different stores. You want to calculate the total sales per store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"Store\": [\"A\", \"A\", \"B\", \"B\", \"C\", \"C\"], \"Sales\": [100, 200, 150, 250, 300, 400]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by 'Store' and sum the 'Sales'\n",
    "total_sales = df.groupby(\"Store\")[\"Sales\"].sum()\n",
    "\n",
    "print(\"Total sales per store:\")\n",
    "total_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(laptops.groupby(\"category\", observed=True).groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get maximum price for each category\n",
    "max_prices = laptops.groupby(\"category\", observed=True)[\"price_euros\"].max()\n",
    "max_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform multiple operations on grouped data at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dif(group):\n",
    "    return group.max() - group.min()\n",
    "\n",
    "\n",
    "laptops.groupby(\"category\", observed=True)[\"price_euros\"].agg([\"mean\", \"max\", \"median\", \"std\", dif])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data aggregation is a vital step in the data analysis process, as it transforms large, detailed datasets into meaningful summaries. The split-apply-combine strategy is the conceptual framework behind many aggregation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Are laptops made by Apple more expensive than those made by\n",
    "# other manufacturers?\n",
    "laptops.groupby(\"manufacturer\", observed=False)[\"price_euros\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: What is the best value laptop with a screen size of 38 cm or more?\n",
    "cols_to_show = [\"manufacturer\", \"model_name\", \"price_euros\", \"screen_size_cm\"]\n",
    "best_value_laptop = laptops.loc[laptops[\"screen_size_cm\"] >= 38, cols_to_show].sort_values(\"price_euros\")\n",
    "print(f\"Best value laptop is {best_value_laptop.iloc[0]['manufacturer']} {best_value_laptop.iloc[0]['model_name']} with price {best_value_laptop.iloc[0]['price_euros']} euros\")\n",
    "best_value_laptop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Which laptop has the most RAM?\n",
    "cols_to_show = [\"manufacturer\", \"model_name\", \"ram_gb\"]\n",
    "most_ram_laptop = laptops.loc[laptops[\"ram_gb\"] == laptops[\"ram_gb\"].max(), cols_to_show]\n",
    "most_ram_laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Data: Melting and Pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​In data analysis, the structure of your dataset plays a crucial role in determining the ease and effectiveness of your analysis. Often, data needs to be reshaped to fit the requirements of specific analytical methods or visualization tools. Pandas, a powerful data manipulation library in Python, provides robust functions—namely `melt()` and `pivot_table()` — to facilitate the transformation between different data formats.​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`pandas.pivot_table`](https://pandas.pydata.org/docs/reference/api/pandas.pivot_table.html): Create a spreadsheet-style pivot table as a DataFrame. The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame.\n",
    "\n",
    "The <code>df.pivot_table()</code> method can perform the same kinds of aggregations as the <code>df.groupby</code> method and make the code for complex aggregations easier to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the groupby method to get the mean price for each category\n",
    "laptops.groupby(\"category\", observed=True)[\"price_euros\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but using pivot_table\n",
    "laptops_pivot_category = laptops.pivot_table(values=\"price_euros\", index=\"category\", aggfunc=\"mean\", observed=False)\n",
    "laptops_pivot_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that this method returns a dataframe, so normal dataframe filtering and methods can be applied to the result. For example, let's use the DataFrame.plot() method to create a visualization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_pivot_category.plot(\n",
    "    kind=\"barh\",\n",
    "    figsize=(10, 6),\n",
    "    color=\"skyblue\",\n",
    "    title=\"Mean Price by Category\",\n",
    "    xlabel=\"Price (Euros)\",\n",
    "    ylabel=\"Category\",\n",
    "    legend=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.pivot_table(values=[\"price_euros\", \"weight_kg\"], index=\"category\", aggfunc=\"mean\", observed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.pivot_table(\n",
    "    values=[\"price_euros\", \"weight_kg\"],\n",
    "    index=[\"manufacturer\", \"category\"],\n",
    "    aggfunc=[\"mean\", \"max\", \"min\"],\n",
    "    observed=False,\n",
    "    margins=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`pandas.DataFrame.melt`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.melt.html#pandas.DataFrame.melt): Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are “unpivoted” to the row axis, leaving just two non-identifier columns, ‘variable’ and ‘value’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.melt(id_vars=[\"model_name\", \"category\"], value_vars=[\"ram_gb\", \"weight_kg\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End EDA Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practical example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
